{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Speaker Diarization and Audio Transcription in Python\n",
        "## Introduction\n",
        "This notebook provides a Python solution for downloading a YouTube video's audio, transcribing the audio, and performing speaker diarization. The output is analyzed to provide a consolidated output in a Pandas DataFrame. Speaker diarization is the process of separating an audio stream into segments, each associated with a different speaker. Audio transcription is the process of converting speech to text. \n",
        "\n"
      ],
      "metadata": {
        "id": "TkMxElEJDRbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##Installation of libraries\n",
        "!pip install pydub\n",
        "!pip install torch\n",
        "!pip install torchaudio\n",
        "!pip install -U demucs\n",
        "!pip install -U stable-ts\n",
        "!pip install geopandas\n",
        "!pip install yt-dlp\n",
        "!pip install -qq https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XQQSB90VNgzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Load of the models\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "from shapely.geometry import LineString\n",
        "from stable_whisper import load_model\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "hugging_face_token = \"\" #@param {type:\"string\"}\n",
        "# Load the Stable Whisper model and Pyannote Pipeline\n",
        "model = load_model('large-v2')\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
        "                                    use_auth_token=hugging_face_token)"
      ],
      "metadata": {
        "id": "kfWLvhL8NkH4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tlFNt4uoB_Mb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ##Define functions to use\n",
        "def download_audio(url, output_filename):\n",
        "    \"\"\"\n",
        "    Download and convert YouTube audio to a mono MP3 file.\n",
        "\n",
        "    Args:\n",
        "    url (str): YouTube video URL\n",
        "    output_filename (str): Output MP3 file name with .mp3 extension\n",
        "    \"\"\"\n",
        "    # Download YouTube video as audio and convert to MP3\n",
        "    temp_file = \"temp_audio\"\n",
        "    command = f'yt-dlp -x --audio-format vorbis --output \"{temp_file}.%(ext)s\" {url}'\n",
        "    subprocess.call(command, shell=True)\n",
        "\n",
        "    # Find the downloaded file with the correct extension\n",
        "    for file in os.listdir():\n",
        "        if file.startswith(temp_file):\n",
        "            temp_file_with_ext = file\n",
        "            break\n",
        "\n",
        "    # Load the audio file using pydub and convert to mono\n",
        "    audio = AudioSegment.from_file(temp_file_with_ext)\n",
        "    audio = audio.set_channels(1)  # Set to mono\n",
        "    audio.export(output_filename, format='mp3')\n",
        "\n",
        "    # Remove temporary file\n",
        "    os.remove(temp_file_with_ext)\n",
        "    print(f\"Audio downloaded and converted successfully: {output_filename}\")\n",
        "\n",
        "\n",
        "def process_audio(input_audio, output_json, output_lab,demucs=False,vad=False,language=None):\n",
        "    \"\"\"\n",
        "    Process the audio file to obtain transcriptions and speaker diarization.\n",
        "\n",
        "    Args:\n",
        "    input_audio (str): Input audio file name\n",
        "    output_json (str): Output JSON file name for transcriptions\n",
        "    output_lab (str): Output LAB file name for speaker diarization\n",
        "    \"\"\"\n",
        "    # Transcribe audio and save to JSON\n",
        "    result = model.transcribe(input_audio, language=language, demucs=demucs, vad=vad, regroup=False)\n",
        "    result.save_as_json(output_json)\n",
        "\n",
        "    # Perform speaker diarization and save to LAB\n",
        "    diarization_result = pipeline(input_audio)\n",
        "    with open(output_lab, \"w\") as rttm:\n",
        "        diarization_result.write_lab(rttm)\n",
        "\n",
        "def analyze_transcriptions_and_diarization(transcriptions_json, diarization_lab):\n",
        "    \"\"\"\n",
        "    Analyze transcriptions and diarization results.\n",
        "\n",
        "    Args:\n",
        "    transcriptions_json (str): JSON file with transcriptions\n",
        "    diarization_lab (str): LAB file with diarization results\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with analyzed results\n",
        "    \"\"\"\n",
        "    with open(transcriptions_json, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    \n",
        "    wrd = pd.DataFrame(data['segments'])[['id', 'words']].explode('words')\n",
        "    wrd = pd.concat([wrd.drop(['words'], axis=1), wrd['words'].apply(pd.Series)], axis=1)\n",
        "    wrd['id2'] = range(len(wrd))\n",
        "\n",
        "    voice = pd.read_csv(diarization_lab, header=None, delimiter=r\" \")\n",
        "    voice.columns = ['start', 'end', 'speaker']\n",
        "    voice['id2'] = range(len(voice))\n",
        "\n",
        "    wrd['geometry'] = wrd.apply(lambda row: LineString([(row['start'], 0), (row['end'], 0)]), axis=1)\n",
        "    voice['geometry'] = voice.apply(lambda row: LineString([(row['start'], 0), (row['end'], 0)]), axis=1)\n",
        "    wrd, voice = gpd.GeoDataFrame(wrd, geometry='geometry'), gpd.GeoDataFrame(voice, geometry='geometry')\n",
        "\n",
        "    wrd['len'] = wrd.geometry.length\n",
        "    inter = gpd.overlay(wrd, voice, how='union').query('not id.isnull()')\n",
        "    inter = inter.sort_values(by='id2_2').sort_values(by='id2_1').reset_index()\n",
        "    inter['p_voice'] = inter.geometry.length / inter.len\n",
        "    inter = inter.query('not p_voice.isnull()')\n",
        "    inter = inter[['id', 'id2_1', 'speaker', 'word', 'start_1', 'end_1', 'probability', 'p_voice']]\n",
        "    inter['ord'] = (~inter.speaker.isnull()) * 100 + inter.probability * 10 + inter.p_voice\n",
        "    inter = inter.groupby('id2_1', group_keys=False).apply(lambda x: x.nlargest(1, 'ord', keep='all')).reset_index()\n",
        "\n",
        "    tmp = inter.groupby(['id', 'speaker']).apply(lambda x: {\"sentence\": ' '.join(x['word']),\n",
        "                                                            \"start_1\": min(x['start_1']),\n",
        "                                                            \"end_1\": max(x['end_1']),\n",
        "                                                            \"prob\": np.exp(np.average(np.log(x['probability']))),\n",
        "                                                            \"p_voice\": np.exp(np.average(np.log(x['p_voice'])))})\n",
        "    tmp = tmp.apply(pd.Series).sort_values(by='start_1').reset_index(['id', 'speaker'])\n",
        "    return inter,tmp\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example"
      ],
      "metadata": {
        "id": "AcipRYTTE5Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_youtube='https://www.youtube.com/watch?v=pj705DvCSxg' #@param {type:\"string\"}\n",
        "download_audio(url_youtube, 'out.mp3')\n",
        "\n",
        "# Process the audio to obtain transcriptions and speaker diarization\n",
        "process_audio('out.mp3', 'out.json', 'output.lab',language=\"en\")\n",
        "results_df, result_simp = analyze_transcriptions_and_diarization('out.json', 'output.lab')\n",
        "result_simp"
      ],
      "metadata": {
        "id": "tX6ftw-Ks6HD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
